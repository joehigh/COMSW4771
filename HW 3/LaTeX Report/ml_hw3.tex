\documentclass[twoside,11pt]{homework}

\coursename{COMS 4771 Machine Learning 2020} 

\studname{Joseph High}
\studmail{jph2185}
\hwNo{3}
\date{\today} 

% Uncomment the next line if you want to use \includegraphics.
\usepackage{graphicx}
%\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{relsize}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage[dvipsnames]{xcolor}
\usepackage[makeroom]{cancel}
\usepackage{collectbox}
%\usepackage{cleveref}
\usepackage{eqnarray}
\usepackage{titlesec} 
\usepackage{bm} 
\usepackage{hyperref}
\usepackage{flafter}
\usepackage{graphicx}
\usepackage{float}
%\titleformat{\subsubsection}[runin]
%  {\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}

\DeclarePairedDelimiter{\2norm}{\lVert}{\rVert^2_2}
\newcommand{\defeq}{\vcentcolon=}
\newcommand\unif{\ensuremath{\operatorname{unif}}}
\DeclareMathOperator{\EX}{\mathbb{E}}% expected value
\newcommand{\1}[1]{\mathds{1}\left[#1\right]}

\makeatletter
\newcommand{\mybox}{%
    \collectbox{%
        \setlength{\fboxsep}{1pt}%
        \fbox{\BOXCONTENT}%
    }%
}


%%%%%%%%%%%%%%%% Direct Comments %%%%%%%%%%%%%%%%
\newcommand{\joe}[1]{\textcolor{yellow}{\colorbox{blue}{\parbox{15.5cm}{\textbf{\textsc{Joe}: \ #1}}}}}
%newcommand{\joe}[1]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\subsection*{Problem 1: Non-parametric Regression via Bayesian Modelling}
\vspace{0.2in}

\subsubsection*{Part (i)} 
$\textbf{x} =  \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \sim \mathcal{N}\left(\begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix}, \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{bmatrix} \right)$ if and only if $x_i \sim \mathcal{N}\left(x_i; \mu_i, \Sigma_{ii}\right)$ for $ i = 1, 2$.\footnote{Larry Wasserman, \textit{All of Statistics: A Concise Course in Statistical Inference} (New York: Springer, 2004), 40, Theorem 2.44.} \\\ \\\ \\\
Therefore, $x_1 \sim \mathcal{N}\left(x_1; \mu_1, \Sigma_{11}\right)$. A proof of this fact is provided below. \\\ \\\
We can derive the marginal distribution of $x_1$ by integrating out $x_2$.\\ \\
%Given that \ $\textbf{x} =  \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \sim \mathcal{N}\left(\begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix}, \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{bmatrix} \right)$ 
We are given that $\textbf{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \in \mathbb{R}^d$. Then, $w.l.o.g.$, let $x_1 \in \mathbb{R}^m$ and $x_2 \in \mathbb{R}^n$ where $d = m + n$. That is, the dimensions of $x_1$ and $x_2$ should sum to the dimension of $\textbf{x}$. \\\ \\\
Letting \ $\begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{bmatrix}^{-1} = \begin{bmatrix} \Lambda_{11} & \Lambda_{12} \\ \Lambda_{21} & \Lambda_{22} \end{bmatrix} $  
and $c = \mathlarger{(2\pi)^{d/2}|\Sigma|^{1/2}}$, we have the following:

\begin{align*}
f_{\textbf{x}}(x_1) & = \ \mathlarger{\frac{1}{c} \int} \text{exp} \left\lbrace-\frac{1}{2}\left(\begin{bmatrix} x_1 - \mu_1 \\ x_2 - \mu_2 \end{bmatrix}^{\top} \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{bmatrix}^{-1} \begin{bmatrix} x_1 - \mu_1 \\ x_2 - \mu_2 \end{bmatrix}\right) \right\rbrace dx_2 \\
& = \ \mathlarger{\frac{1}{c} \int} \text{exp} \left\lbrace-\frac{1}{2}\left(\begin{bmatrix} x_1 - \mu_1 \\ x_2 - \mu_2 \end{bmatrix}^{\top} \begin{bmatrix} \Lambda_{11} & \Lambda_{12} \\ \Lambda_{21} & \Lambda_{22} \end{bmatrix} \begin{bmatrix} x_1 - \mu_1 \\ x_2 - \mu_2 \end{bmatrix}\right) \right\rbrace dx_2 \\
& = \ \mathlarger{\frac{1}{c} \int} \text{exp} \left\lbrace-\left[ \frac{1}{2}(x_1 - \mu_1)^{\top}\Lambda_{11}(x_1-\mu_1)  +  \frac{1}{2}(x_2 - \mu_2)^{\top}\Lambda_{21}(x_1-\mu_1) \ + \right. \right. \\
& \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left. \left. + \ \frac{1}{2}(x_1 - \mu_1)^{\top}\Lambda_{12}(x_2-\mu_2) + \frac{1}{2}(x_2 - \mu_2)^{\top}\Lambda_{22}(x_2-\mu_2)\right] \right\rbrace dx_2 \\
\end{align*}
\begin{align*}
& = \ \mathlarger{\frac{1}{c} \int} \text{exp} \left\lbrace-\left[ \frac{1}{2}(x_1 - \mu_1)^{\top}\Lambda_{11}(x_1-\mu_1)  +  (x_1 - \mu_1)^{\top}\Lambda_{12}(x_2-\mu_2) \ + \right. \right. \\
& \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left. \left. + \ \frac{1}{2}(x_2 - \mu_2)^{\top}\Lambda_{22}(x_2-\mu_2)\right] \right\rbrace dx_2 \\
\intertext{\emph{completing the square...}}
\ \ \ \ \ \ \ \ & =  \ \mathlarger{\frac{1}{c} \int} \text{exp} \left\lbrace-\left[ \frac{1}{2}\left((x_2 - \mu_2) +\Lambda_{22}^{-1}\Lambda_{21}(x_1-\mu_1)\right)^{\top} \Lambda_{22}\left((x_2-\mu_2) + \Lambda_{22}^{-1}\Lambda_{21}(x_1-\mu_1)\right)  + \right. \right. \\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left. \left. + \ \frac{1}{2}(x_1-\mu_1)^{\top}\Lambda_{11}( x_1-\mu_1) - \frac{1}{2}(x_1-\mu_1)^{\top}\Lambda_{12}\Lambda_{22}^{-1}\Lambda_{21}( x_1-\mu_1) \right] \right\rbrace dx_2 \\
& = \ \text{exp}\left\lbrace -\frac{1}{2}(x_1-\mu_1)^{\top}\Lambda_{11}( x_1-\mu_1) - \frac{1}{2}(x_1-\mu_1)^{\top}\Lambda_{12}\Lambda_{22}^{-1}\Lambda_{21}( x_1-\mu_1) \right\rbrace \cdot\\
& \cdot \frac{1}{c}\underbrace{\mathlarger{\int}\text{exp}\left\lbrace  -\left( \frac{1}{2}\left[(x_2 - \mu_2) +\Lambda_{22}^{-1}\Lambda_{21}(x_1-\mu_1)\right]^{\top} \Lambda_{22}\left[(x_2-\mu_2) + \Lambda_{22}^{-1}\Lambda_{21}(x_1-\mu_1)\right] \right) \right\rbrace dx_2}_{= \ (2\pi)^{n/2}|\Lambda_{22}^{-1}|^{1/2}} \\
& = \ \text{exp}\left\lbrace -\frac{1}{2}(x_1-\mu_1)^{\top}\left(\Lambda_{11} - \Lambda_{12}\Lambda_{22}^{-1}\Lambda_{21}\right)( x_1-\mu_1) \right\rbrace \cdot \frac{1}{c}(2\pi)^{n/2}|\Lambda_{22}^{-1}|^{1/2}\\[1em]
& = \ \frac{(2\pi)^{n/2}|\Lambda_{22}^{-1}|^{1/2}}{(2\pi)^{d/2}|\Sigma|^{1/2}} \cdot \text{exp}\left\lbrace -\frac{1}{2}(x_1-\mu_1)^{\top}\Sigma_{11}^{-1}( x_1-\mu_1) \right\rbrace  \\[1em]
& =  \ \frac{(2\pi)^{n/2}|\Sigma_{22} - \Sigma_{12}^{\top}\Sigma_{11}^{-1}\Sigma_{12}|^{1/2}}{(2\pi)^{d/2}\left(|\Sigma_{11}||\Sigma_{22} - \Sigma_{12}^{\top}\Sigma_{11}^{-1}\Sigma_{12}|\right)^{1/2}} \cdot \text{exp}\left\lbrace -\frac{1}{2}(x_1-\mu_1)^{\top}\Sigma_{11}^{-1}( x_1-\mu_1) \right\rbrace   \ \ \ \ \ \ \ \ \text{\footnotesize \emph{(from part (ii))}} \\[1em]
& = \ \frac{1}{(2\pi)^{m/2}|\Sigma_{11}|^{1/2}} \cdot \text{exp}\left\lbrace -\frac{1}{2}(x_1-\mu_1)^{\top}\Sigma_{11}^{-1}( x_1-\mu_1) \right\rbrace     \\[1em]
& = \ \mathcal{N}\left(x_1; \mu_1, \Sigma_{11}\right) 
\end{align*}
\text{}

Hence, \ $x_1 \sim \mathcal{N}\left(x_1; \mu_1, \Sigma_{11}\right)$

\newpage

\subsubsection*{Part (ii)} 
\begin{proof}
The joint distribution on \ $\textbf{x} =  \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \sim \mathcal{N}\left(\bm{\mu}, \Sigma \right) \ = \ \mathcal{N}\left(\begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix}, \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{bmatrix} \right)$, \  is 
% Joint distribution
\begin{equation} \label{eq:joint}
f(\textbf{x}) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \ \text{exp} \left\lbrace -\frac{1}{2}\left(\textbf{x} - \bm{\mu}\right)^{\top} \Sigma^{-1} \left(\textbf{x} - \bm{\mu}\right) \right\rbrace
\end{equation}
To express (\ref{eq:joint}) in the desired form, the following facts which were provided in the homework handout will be used:

% Facts given in HW 3 handout
\begin{enumerate}[a.)]
\item $ \Sigma^{-1} = \begin{bmatrix} \Sigma^{11} & \Sigma^{12} \\ \Sigma^{21} & \Sigma^{22} \end{bmatrix}$

% Sigma^{11}
\item $\Sigma^{11} = \left(\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}^{\top}\right)^{-1} =  \Sigma_{11}^{-1} + \Sigma_{11}^{-1}\Sigma_{12}\left(\Sigma_{22} - \Sigma_{12}^{\top}\Sigma_{11}^{-1}\Sigma_{12}\right)^{-1}\Sigma_{12}^{\top}\Sigma_{11}^{-1}  $

% Sigma^{22}
\item $\Sigma^{22} = \left(\Sigma_{22} - \Sigma_{12}^{\top}\Sigma_{11}^{-1}\Sigma_{12}\right)^{-1} =  \Sigma_{22}^{-1} + \Sigma_{22}^{-1}\Sigma_{12}^{\top}\left(\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}^{\top}\right)^{-1}\Sigma_{12}\Sigma_{22}^{-1}  $

% Sigma^{12}
\item $\Sigma^{12}  = - \Sigma_{11}^{-1}\Sigma_{12}\left(\Sigma_{22} - \Sigma_{12}^{\top}\Sigma_{11}^{-1}\Sigma_{12}\right)^{-1} = \left(\Sigma_{21}\right)^{\top}  $

%  A matrix
\item  $ A = \Sigma_{22} - \Sigma_{12}^{\top}\Sigma_{11}^{-1}\Sigma_{12}  =  (\Sigma^{22})^{-1}$

% b vector
\item $ b = \mu_2 + \Sigma_{12}^{\top}\Sigma_{11}^{-1}(x_1 - \mu_1)$
\end{enumerate}
From (\ref{eq:joint}), we have
\begin{align*}
f(\textbf{x}) & = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \ \text{exp} \left\lbrace -\frac{1}{2}\begin{bmatrix} x_1 - \mu_1 \\ x_2 - \mu_2 \end{bmatrix}^{\top} \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{bmatrix}^{-1} \begin{bmatrix} x_1 - \mu_1 \\ x_2 - \mu_2 \end{bmatrix} \right\rbrace \\
& = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \ \text{exp} \left\lbrace -\frac{1}{2}\begin{bmatrix} x_1 - \mu_1 \\ x_2 - \mu_2 \end{bmatrix}^{\top} \begin{bmatrix} \Sigma^{11} & \Sigma^{12} \\ \Sigma^{21} & \Sigma^{22} \end{bmatrix} \begin{bmatrix} x_1 - \mu_1 \\ x_2 - \mu_2 \end{bmatrix} \right\rbrace  \\
& = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \ \text{exp} \left\lbrace -\frac{1}{2} \left[(x_1 - \mu_1)^{\top}\Sigma^{11}(x_1-\mu_1)  +  (x_2 - \mu_2)^{\top}\Sigma^{21}(x_1-\mu_1) \right. \right. \\[1em]
&\quad  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ + \ (x_1 - \mu_1)^{\top}\Sigma^{12}(x_2-\mu_2) + (x_2 - \mu_2)^{\top}\Sigma^{22}(x_2-\mu_2)\Big] \Bigg\} \\[1em]
& = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \ \text{exp} \left\lbrace -\frac{1}{2} \left[(x_1 - \mu_1)^{\top}\Sigma^{11}(x_1-\mu_1)  +  2(x_1 - \mu_1)^{\top}\Sigma^{12}(x_2-\mu_2) \right. \right. \\[1em]
&\quad  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ + \ (x_2 - \mu_2)^{\top}\Sigma^{22}(x_2-\mu_2)\Big] \Bigg\}  \\[1em]
\end{align*}
Focusing only on the exponent and substituting in expressions (b), (c), (d), and (e) we get
\begin{align*}
& -\frac{1}{2}(x_1 - \mu_1)^{\top}\Sigma^{11}(x_1-\mu_1) \ - \ \frac{1}{2}(x_2 - \mu_2)^{\top}\Sigma^{21}(x_1-\mu_1)  - \frac{1}{2}(x_1 - \mu_1)^{\top}\Sigma^{12}(x_2-\mu_2) \\
&\quad \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ - \ \frac{1}{2}(x_2 - \mu_2)^{\top}\Sigma^{22}(x_2-\mu_2) \\[1em]
& = \ -\frac{1}{2}(x_1 - \mu_1)^{\top}{\color{blue} \underbrace{\color{black} \left(\Sigma_{11}^{-1} + \Sigma_{11}^{-1}\Sigma_{12}\left(\Sigma_{22} - \Sigma_{12}^{\top}\Sigma_{11}^{-1}\Sigma_{12}\right)^{-1}\Sigma_{12}^{\top}\Sigma_{11}^{-1}\right)}_{\mathlarger{= \ \Sigma^{11}}}}(x_1-\mu_1)  \\[1em]
 &\quad - \ \frac{1}{2}(x_2 - \mu_2)^{\top}{\color{blue} \underbrace{\color{black} \left(- \Sigma_{11}^{-1}\Sigma_{12}\left(\Sigma_{22} - \Sigma_{12}^{\top}\Sigma_{11}^{-1}\Sigma_{12}\right)^{-1}\right)^{\top}}_{\mathlarger{= \ \Sigma^{21}}}}(x_1-\mu_1) \\[1em]
 &\quad - \frac{1}{2}(x_1 - \mu_1)^{\top}{\color{blue} \underbrace{\color{black} \left(-\Sigma_{11}^{-1}\Sigma_{12}\left(\Sigma_{22} - \Sigma_{12}^{\top}\Sigma_{11}^{-1}\Sigma_{12}\right)^{-1}\right)}_{\mathlarger{= \ \Sigma^{12}}}}(x_2-\mu_2) - \ \frac{1}{2}(x_2 - \mu_2)^{\top}{\color{blue} \underbrace{\color{black} A^{-1}}_{\mathlarger{= \ \Sigma^{22}}}}(x_2-\mu_2) \\
 \intertext{\emph{(Substituting in $A = \Sigma_{22} - \Sigma_{12}^{\top}\Sigma_{11}^{-1}\Sigma_{12} $)}}
 & = \ -\frac{1}{2}(x_1 - \mu_1)^{\top} \left(\Sigma_{11}^{-1} + \Sigma_{11}^{-1}\Sigma_{12}A^{-1}\Sigma_{12}^{\top}\Sigma_{11}^{-1}\right)(x_1-\mu_1) \ + \ \frac{1}{2}(x_2 - \mu_2)^{\top} \left(\Sigma_{11}^{-1}\Sigma_{12}A^{-1}\right)^{\top}(x_1-\mu_1) \\[0.5em]
&\quad \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ + \ \frac{1}{2}(x_1 - \mu_1)^{\top} \left(\Sigma_{11}^{-1}\Sigma_{12}A^{-1}\right)(x_2-\mu_2) - \ \frac{1}{2}(x_2 - \mu_2)^{\top}A^{-1}(x_2-\mu_2) \\[1em]
& = \ -\frac{1}{2}(x_1 - \mu_1)^{\top}\Sigma_{11}^{-1}(x_1-\mu_1) \ - \ \frac{1}{2}(x_1 - \mu_1)^{\top}\Sigma_{11}^{-1}\Sigma_{12}A^{-1}\Sigma_{12}^{\top}\Sigma_{11}^{-1}(x_1-\mu_1) \\[0.5em]
&\quad \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ + (x_1 - \mu_1)^{\top} \left(\Sigma_{11}^{-1}\Sigma_{12}A^{-1}\right)(x_2-\mu_2) \ - \ \frac{1}{2}(x_2 - \mu_2)^{\top}A^{-1}(x_2-\mu_2) \\[0.8em]
\intertext{\emph{Factor the last three summands using the fact that $(v-w)^{\top}S(v-w) = v^{\top}Sv - 2w^{\top}Sv + w^{\top}Sw$ for any vectors $v$ and $w$ and any symmetric matrix $S$.}} 
& = \ -\frac{1}{2}(x_1 - \mu_1)^{\top}\Sigma_{11}^{-1}(x_1-\mu_1) \\[0.5em]
&\quad \ \ \ \ \ \ \ - \ \frac{1}{2}\left[(x_2 - \mu_2) - \Sigma_{12}^{\top}\Sigma_{11}^{-1}(x_1-\mu_1) \right]^{\top} A^{-1} \left[(x_2 - \mu_2) - \Sigma_{12}^{\top}\Sigma_{11}^{-1}(x_1-\mu_1)\right] \\[1em]
& = \ -\frac{1}{2}(x_1 - \mu_1)^{\top}\Sigma_{11}^{-1}(x_1-\mu_1) \\[0.5em]
&\quad \ \ \ \ \ \ \ - \ \frac{1}{2}\left[x_2 - \left(\mu_2 + \Sigma_{12}^{\top}\Sigma_{11}^{-1}(x_1-\mu_1)\right)\right]^{\top} A^{-1} \left[x_2 - \left(\mu_2 + \Sigma_{12}^{\top}\Sigma_{11}^{-1}(x_1-\mu_1)\right)\right]  \\[1em]
\end{align*}
$ = \ \mathlarger{-\frac{1}{2}(x_1 - \mu_1)^{\top}\Sigma_{11}^{-1}(x_1-\mu_1) + \ \frac{1}{2}\left[\left(x_2 - b \right)^{\top} A^{-1} \left(x_2 - b \right)  \right]}$\\\ \\\ \\\
Therefore, the joint distribution of $\textbf{x}$ can be written as 
$$ \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \ \text{exp} \left\lbrace -\frac{1}{2}\left((x_1 - \mu_1)^{\top}\Sigma_{11}^{-1}(x_1-\mu_1) \ + \ \left(x_2 - b \right)^{\top} A^{-1} \left(x_2 - b \right)\right) \right\rbrace$$
\end{proof}
\vspace{0.1in}
\subsubsection*{Part (iii)} 
\begin{proof}
We are given that $\textbf{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \in \mathbb{R}^d$. Then $w.l.o.g.$, suppose $x_1 \in \mathbb{R}^m$ and $x_2 \in \mathbb{R}^n$ where $d = m + n$. That is, the dimensions of $x_1$ and $x_2$ should sum to the dimension of $\textbf{x}$. \\\ \\\
To show that the joint on $\textbf{x}$ can be decomposed as the product\ $\mathcal{N}\left(x_1; \mu_1, \Sigma_{11}\right)\mathcal{N}\left(x_2; b, A\right)$, we can continue from the result in part (ii):
\begin{align*} \hspace{-3cm}
& f(x_1, x_2) =  \\[1em]
& \ \ \ \ \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \ \text{exp} \left\lbrace -\frac{1}{2}\left((x_1 - \mu_1)^{\top}\Sigma_{11}^{-1}(x_1-\mu_1) \ + \ \left(x_2 - b \right)^{\top} A^{-1} \left(x_2 - b \right)\right) \right\rbrace \\[1em]
& = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \ \text{exp} \left\lbrace -\frac{1}{2}(x_1 - \mu_1)^{\top}\Sigma_{11}^{-1}(x_1-\mu_1) \ - \ \frac{1}{2}\left(x_2 - b \right)^{\top} A^{-1} \left(x_2 - b \right) \right\rbrace \\[1em]
& = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \ \text{exp} \left\lbrace -\frac{1}{2}(x_1 - \mu_1)^{\top}\Sigma_{11}^{-1}(x_1-\mu_1) \right\rbrace \text{exp} \left\lbrace -\frac{1}{2}\left(x_2 - b \right)^{\top} A^{-1} \left(x_2 - b \right) \right\rbrace \\[1em]
\intertext{\footnotesize \textit{Using \ $|\Sigma| = |\Sigma_{11}||\Sigma_{22} - \Sigma_{12}^{\top}\Sigma_{11}^{-1}\Sigma_{12}|$ and $d = m+n$ we have:}}
& = \frac{1}{(2\pi)^{\frac{m+n}{2}}\left(|\Sigma_{11}||\Sigma_{22} - \Sigma_{12}^{\top}\Sigma_{11}^{-1}\Sigma_{12}|\right)^{\frac{1}{2}}} \ \text{exp} \left\lbrace -\frac{1}{2}(x_1 - \mu_1)^{\top}\Sigma_{11}^{-1}(x_1-\mu_1) \right\rbrace \\[0.1em]
&\quad \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \cdot \text{exp} \left\lbrace -\frac{1}{2}\left(x_2 - b \right)^{\top} A^{-1} \left(x_2 - b \right) \right\rbrace \\[0.5em]
\intertext{\footnotesize \textit{From the supposition in part (ii), \ A = $\Sigma_{22} - \Sigma_{12}^{\top}\Sigma_{11}^{-1}\Sigma_{12}$. Plugging in, we get:}} \\
& = \frac{1}{(2\pi)^{\frac{m}{2}}(2\pi)^{\frac{n}{2}}|\Sigma_{11}|^{\frac{1}{2}}|A|^{\frac{1}{2}}} \ \text{exp} \left\lbrace -\frac{1}{2}(x_1 - \mu_1)^{\top}\Sigma_{11}^{-1}(x_1-\mu_1) \right\rbrace \text{exp} \left\lbrace -\frac{1}{2}\left(x_2 - b \right)^{\top} A^{-1} \left(x_2 - b \right) \right\rbrace \\[1em]
\end{align*}
\begin{align*}
& = \underbrace{\frac{1}{(2\pi)^{m/2}|\Sigma_{11}|^{1/2}} \ \text{exp} \left\lbrace -\frac{1}{2}(x_1 - \mu_1)^{\top}\Sigma_{11}^{-1}(x_1-\mu_1) \right\rbrace}_{\color{blue} = \ \mathcal{N}(x_1; \mu_1, \Sigma_{11})} \cdot \underbrace{ \frac{1}{(2\pi)^{n/2}|A|^{1/2}} \text{exp} \left\lbrace -\frac{1}{2}\left(x_2 - b \right)^{\top} A^{-1} \left(x_2 - b \right) \right\rbrace}_{\color{blue} = \ \mathcal{N}(x_2; b, A)} \\[2em]
& = \ \mathcal{N}(x_1; \mu_1, \Sigma_{11})\mathcal{N}(x_2; b, A)
\end{align*}
\end{proof}
%\vspace{0.1in}

\subsubsection*{Part (iv)}
The conditional distribution of $x_2$ given $x_1$ can be computed from the joint distribution on $\textbf{x} = [x_1 \  \ x_2]^{\top}$ and the marginal distribution of $x_1$: $$f(x_2 \vert x_1) = \frac{f(x_1, x_2)}{f(x_1)}$$
Thus, we can use the results from parts (i) and (iii) to compute the density of $x_2 \vert x_1$.\\\ \\\
In part (i) it was shown that the marginal density on $x_1$ is such that $f(x_1) = \mathcal{N}(x_1; \mu_1, \Sigma_{11})$.\\\ \\\
In part (iii) it was shown that the joint density on $\textbf{x}$ can be decomposed into the \\\ 
product \ $\mathcal{N}(x_1; \mu_1, \Sigma_{11})\mathcal{N}(x_2; b, A)$. \\\ \\\
The conditional density of $x_2$ given $x_1$ is then 
$$f(x_2 \vert x_1) \ = \ \frac{f(x_1, x_2)}{f(x_1)} \ = \ \frac{\mathcal{N}(x_1; \mu_1, \Sigma_{11})\mathcal{N}(x_2; b, A)}{\mathcal{N}(x_1; \mu_1, \Sigma_{11})} \ = \ \mathcal{N}(x_2; b, A)$$ \\\ 
$\Longrightarrow \ \ \ x_2 \vert x_1 \sim \mathcal{N}(x_2; b, A)$ \\\ \\\
or, equivalently (since $b = \mu_2 + \Sigma_{12}^{\top}\Sigma_{11}^{-1}(x_1 - \mu_1)$ \ and \ $A = \Sigma_{22} - \Sigma_{12}^{\top}\Sigma_{11}^{-1}\Sigma_{12}$),\\\ \\\
$\ x_2 \vert x_1 \sim \mathcal{N}\left(x_2; \mu_2 + \Sigma_{12}^{\top}\Sigma_{11}^{-1}(x_1 - \mu_1), \Sigma_{22} - \Sigma_{12}^{\top}\Sigma_{11}^{-1}\Sigma_{12}\right)  $  \ \ \ \ \ \ \ \ \

\newpage

\subsubsection*{Part (v)}
Four random functions were generated in Python using the Gaussian distribution with $\mu_n = \bm{\vec{0}}$ and $\Sigma_{n \times n} = I$ and subsequently plotted (See \autoref{fig:whitenoise}). One will note that these random functions appear similar to that of a white noise process. This makes sense since the random numbers were randomly sampled from a Gaussian distribution such that the correlation between all $x_i$ and $x_j$, for $i, j \in \{1,...,500\} $ and $i \neq  j$, is zero. It's not surprising, then, that the resulting functions do not appear to be smooth at all. 

\begin{figure}[H]
\centering
\caption{Random Functions with $\Sigma_{n \times n} = I$ and $\mu_{n} = \bm{\vec{0}}$}
\includegraphics[scale=0.4]{"../code/figures/part_v_whitenoise"}
\label{fig:whitenoise}
\end{figure}

\noindent
\autoref{fig:ones} (below) displays random functions generated with $\Sigma_{n \times n} = \mathbb{J}_{n}$ (the all ones matrix) and $\mu_{n} = \bm{\vec{0}}$. Notice that each function is a straight line at different values on the $y-$axis, indicating that the function values are perfectly correlated with one another.

\begin{figure}[H]
\centering
\caption{Random Functions with $\Sigma_{n \times n} = \mathbb{J}_{n}$ and $\mu_{n} = \bm{\vec{0}}$}
\includegraphics[scale=0.4]{"../code/figures/part_v_correlated"}
\label{fig:ones}
\end{figure}
\noindent
\autoref{fig:cov01} displays random functions generated with $[\Sigma_{i,j}]_{i \neq j} = 0.1$, $[\Sigma_{i,j}]_{i = j} = 0.2$ and $\mu_{n} = \bm{\vec{0}}$. Here, we see that there is much less noise than that of \autoref{fig:whitenoise} where the functions were generated with zero correlation. While there is some positive correlation between each pair of values, the functions still do not appear to be smooth.

\begin{figure}[H]
\centering
\caption{Random Functions with $[\Sigma_{i,j}]_{i \neq j} = 0.1$, $[\Sigma_{i,j}]_{i = j} = 0.2$ and $\mu_{n} = \bm{\vec{0}}$}
\includegraphics[scale=0.4]{"../code/figures/part_v_mean0_cov01"}
\label{fig:cov01}
\end{figure}
\noindent
\autoref{fig:sigma_vary} displays the functions whereby $\Sigma$ was gradually increased. It comes as no surprise that the resulting behavior is an increase in noise as $\Sigma$ is increased.

\begin{figure}[H]
\centering
\caption{Varying $\Sigma$ and $\mu_{n} = \bm{\vec{0}}$}
\includegraphics[scale=0.39]{"../code/figures/part_v_sigma"}
\label{fig:sigma_vary}
\end{figure}
\noindent
\autoref{fig:mu_vary} displays the functions whereby $\mu$ was gradually increased. Naturally, the functions are centered at wherever $\mu$ is set.

\begin{figure}[H]
\centering
\caption{Varying $\mu$ and setting $[\Sigma_{i,j}]_{i = j} = 0.1$, $[\Sigma_{i,j}]_{i \neq j} = 0$}
\includegraphics[scale=0.39]{"../code/figures/part_v_mu"}
\label{fig:mu_vary}
\end{figure}
\noindent
\autoref{fig:sigma_vary_v2} plots the functions whereby $\Sigma$ is varied. Namely, the diagonal entries, i.e. $[\Sigma_{i,j}]_{i = j}$, were varied from 0.1 to 1, and the off-diagonal entries, i.e. $[\Sigma_{i,j}]_{i \neq j}$, were varied from 0.05 to 2. It can be seen that the function values start off relatively correlated and quickly become wildly uncorrelated. These results make sense in that while the correlation between points are increasing, the variance is also increasing rapidly. 

\begin{figure}[H]
\centering
\caption{Varying $\mu$ and $[\Sigma_{i,j}]_{i = j} = 0.1$, $[\Sigma_{i,j}]_{i \neq j} = 0$}
\includegraphics[scale=0.39]{"../code/figures/part_v_sigma2"}
\label{fig:sigma_vary_v2}
\end{figure}


\subsubsection*{Part (vi)}

\autoref{fig:part_vi} displays the random functions generated using $\Sigma_{n \times n} = K$ and $\mu_{n} = \bm{\vec{0}}$. In this case, each of the random functions appear to be significantly smoother than the functions plotted in part (v).

\begin{figure}[H]
\centering
\caption{Random Functions with $\Sigma_{n \times n} = K$ and $\mu_{n} = \bm{\vec{0}}$}
\includegraphics[scale=0.4]{"../code/figures/part_vi_kernel"}
\label{fig:part_vi}
\end{figure}


\vspace{0.15in}

\subsubsection*{Part (vii)}

The kernel used to generate periodic functions with a periodicity of 3 units was inspired by the periodic covariance covered in \textit{Gaussian Processes for Machine Learning}.\footnote{Carl Rasmussen and Christopher Williams, \textit{Gaussian Processes for Machine Learning} (The MIT Press, 2006), 92} Namely, the squared-exponential kernel was implemented: $$k(x_i, x_j) = \text{exp}\left(-\frac{2\sin^2\left(\frac{x_i-x_j}{2}\right)}{\lambda^2}\right)$$ where $\lambda$ is a hyperparameter which defines the periodicity. To achieve a periodicity of 3 units,  set $\lambda = 3$. \\\ \\\
\autoref{fig:part_vii} displays each random function generated with $\Sigma = k(x_i, x_j)$, as defined above, and $\bm{\mu} = \bm{\vec{0}}$.  One will note that each function clearly exhibits periodic behavior.

\begin{figure}[H]
\centering
\caption{Periodic Random Functions}
\includegraphics[scale=0.4]{"../code/figures/part_vii"}
\label{fig:part_vii}
\end{figure}




\vspace{0.15in}

\subsubsection*{Part (viii)}

Using the result from part(iv), the posterior distribution of $\textbf{Y}\vert\overline{\textbf{Y}}$ is 
$$\mathcal{N}\left(\textbf{Y}; \ \mu_n  +  K(\textbf{X}, \overline{\textbf{X}})K(\overline{\textbf{X}} , \overline{\textbf{X}})^{-1}\left(\overline{\textbf{Y}} - \mu_m\right)  , \ K(\textbf{X}, \textbf{X}) - K(\textbf{X}, \overline{\textbf{X}})K(\overline{\textbf{X}} , \overline{\textbf{X}})^{-1} K(\overline{\textbf{X}},\textbf{X})  \right)$$

\vspace{0.1in}


\subsubsection*{Part (ix)}

\autoref{fig:part_ix} displays the plots for all four functions generated using the kernel $\mathlarger{\text{exp}\left\lbrace\frac{(x_i - x_j)^2}{h}\right\rbrace}$ with $h = 5$. \\\ \\\
As in parts (vi) and (vii) where a kernel was also utilized, the random functions appear to be smooth. The three training data points are plotted in blue. Notice that while all four random functions take on different values for most values of x, each passes through all three training data points. 



\begin{figure}[H]
\centering
\caption{Random Functions with $\Sigma_{n \times n} = K$ (with training data)}
\includegraphics[scale=0.4]{"../code/figures/part_ix"}
\label{fig:part_ix}
\end{figure}


\vspace{0.15in}

\subsubsection*{Part (x)}
As discussed in part(vii) the kernel used to generate periodic functions of periodicity 3 units is: $$k(x_i, x_j) = \text{exp}\left(-\frac{2\sin^2\left(\frac{x_i-x_j}{2}\right)}{\lambda^2}\right)$$ where $\lambda$ is a hyperparameter which defines the periodicity.\footnote{Carl Rasmussen and Christopher Williams, \textit{Gaussian Processes for Machine Learning} (The MIT Press, 2006), 92} To achieve a periodicity of 3 units, $\lambda$ was set to 3. \\\ \\\
\autoref{fig:part_x} displays the plots for all four functions generated using the squared-exponential periodic kernel. 
The three training data points are plotted in blue. Each of the random functions are almost indistinguishable; they're practically superimposed onto one another when plotted in the same graph. Notice that while all four random functions take on different values for most values of x, all four random functions pass through all three training data points. 

\begin{figure}[H]
\centering
\caption{Periodic Random Functions plotted together}
\includegraphics[scale=0.4]{"../code/figures/part_x_all4"}
\label{fig:part_x}
\end{figure}
\noindent
Each of the four plots were also plotted individually to confirm that all four are indeed very similar (see \autoref{fig:part_x}).

\begin{figure}[H]
\centering
\caption{Periodic Random Functions plotted individually}
\includegraphics[scale=0.4]{"../code/figures/part_x_individual"}
\label{fig:part_x}
\end{figure}




\vspace{0.15in}

\subsubsection*{Part (xi)}

The mean of the posterior $\textbf{Y}\vert\overline{\textbf{Y}}$ is 
\begin{align*}
\mu_{\textbf{Y}\vert\overline{\textbf{Y}}} \ & = \  \mu_n  +  K(\textbf{X}, \overline{\textbf{X}})K(\overline{\textbf{X}} , \overline{\textbf{X}})^{-1}\left(\overline{\textbf{Y}} - \mu_m\right) \\[1em]
& = \ K(\textbf{X}, \overline{\textbf{X}})K(\overline{\textbf{X}} , \overline{\textbf{X}})^{-1}\overline{\textbf{Y}} && \left(\text{since } \mu_n = \vec{0} \text{ and } \mu_m = \vec{0}\right)
\end{align*}



\vspace{0.15in}

\subsubsection*{Part (xii)}
\autoref{fig:part_xii} and \autoref{fig:part_xii_periodic} are plots of the mean functions from parts (ix) and (x), respectively.

\begin{figure}[H]
\centering
\caption{Mean Function from part (ix)}
\includegraphics[scale=0.35]{"../code/figures/part_xii_posterior"}
\label{fig:part_xii}
\end{figure}
\vspace{-0.2in}
\begin{figure}[H]
\centering
\caption{Mean Function from part (x)}
\includegraphics[scale=0.35]{"../code/figures/part_xii_periodic"}
\label{fig:part_xii_periodic}
\end{figure}



\end{document} 
