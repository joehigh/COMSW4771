Question 5
================

There are several gradient based optimization procedures that have been proposed
over the years for finding local optima of a smooth non-convex function. A few
of them are listed below.

• (Full) Gradient Descent (GD)
• Stochastic Gradient Descent (SGD)
• SGD with Momentum (SGDM)
• Adaptive Gradient (AdaGrad)
• RMSProp
• AdaDelta
• Adaptive Momentum (ADAM)

Using online resources, books, etc. your goal is to learn about what each technique is about and
what makes each technique different. (make sure to properly cite all resources used!)
(i) Describe each of the techniques in detail. Compare and contrast the relative advantages and
disadvantages of each of the techniques. Are there specific types of datasets
where one technique is expected to perform better than the other? if so, why?
Provide detailed justifications for each.

(ii) Implement each of the optimization procedures and experimentally verify your findings in
part (i). Using synthetic dataset(s) compare and contrast the relative performance of each of
the techniques.
